{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics / Natural Language Processing\n",
    "Using text-based data like comments from employee listening surveys, performance review write-ups, development goals, and many other data sources in the HR world are often underutilized, or not utilized at all.  If an HR function is utilizing the text-based data today, many are having their employees go through the comments and manually coding them.  This is extremely time consuming for larger companies and introduces human bias into the analysis from the start.  In this chapter, I‚Äôll walk through the techniques you can use to turn your text-based data into data you can use for analysis or predictive modeling.\n",
    "\n",
    "I‚Äôll start with commenting on what the various names for text analytics are.  In the data science world, it is referenced to as Natural Language Processing (NLP).  In more traditional contexts it is referred to as text mining.  For the purposes of this chapter, I‚Äôll refer to it as NLP as the term is easier to reference üòä\n",
    "\n",
    "When hearing about NLP, many people think it is this extremely complex topic that has to either be vended out (companies that offer this service will certainly tell you that) or done by data scientists.  In the HR space, this is particularly true given the lack of advanced analytics skillsets that organization has traditionally had.  I‚Äôm here to tell you actually executing the basics of NLP are incredibly easy!  So easy it‚Äôs a bit scary how little you actually would need to understand what is occurring on the backend to use it.  At its core, a basic NLP approach is to take a word or string of words from your text-based data and turn them into columns.  While there are nuances that need to be considered, requiring some knowledge of what the algorithm is doing, much of the complexity is abstracted away by the algorithm.  If you're looking for your faith to be restored in humanity, the people who own and contribute to open source projects like sklearn are a good place to start.  The hard work people put into these open source projects makes the lives of people trying to utilize concepts like NLP significantly easier.\n",
    "\n",
    "#### About the data for this chapter\n",
    "Luckily there are a few HR specific text-based data sources online to pick from for this chapter.  While I was able to get away with generating a dataset for the turnover prediction model chapter, I wouldn‚Äôt have been able to do the same for a text-based dataset.  While an employee engagement survey would have been the ideal type of text-based dataset, [Kaggle has a dataset containing job posting information]( https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction).  While you may not have a direct use case in your role pertaining to job postings, this dataset will provide use an HR focused example to run the NLP techniques on.\n",
    "\n",
    "#### Objective with this data\n",
    "The use case for this chapter will be to categorize the various job postings into similar postings using a topic model.\n",
    "\n",
    "#### Outline of chapter\n",
    "1. We‚Äôll briefly discuss when you need to clean your data beforehand and when you can simply let the algorithm take care of it.\n",
    "\n",
    "2. Vectorizing our data, or splitting our data into the various columns.  This is the ‚Äúcore‚Äù of the NLP process we will utilize.  This section has the most parameter exploration, as the vectorization process is where we have the most options to adjust our process.\n",
    "\n",
    "3. Examining our output and understanding what it means.\n",
    "\n",
    "4. How can we utilize our output?  We can leave it as is and put it into an existing model or analysis, or we can leverage a topic model.\n",
    "\n",
    "5. Topic model approaches and concept explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning: to clean or not to clean?\n",
    "This will be one of the few times anyone tells you you‚Äôre able to err on the side of not cleaning your data vs. cleaning your data (gasp).  If your data is already structured, meaning its already in a cell in an excel or csv file, you probably won‚Äôt need to do any additional cleaning.  This is because the vectorizing step has mechanisms to remove stop words and words that don‚Äôt occur very frequently or words that occur very frequently.  \n",
    "\n",
    "The instances you will likely need to do some data cleaning is if your data already has HTML tags in it for display purposes, or if you‚Äôre pulling data from a word file.  I have had to deal with both and it can take some time, but there are plenty of resources available online for handling this.  At least in the beginning, we will not tackle the steps required for cleaning your data from a word file or when it contains HTML tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>salary_range</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Marketing Intern</td>\n",
       "      <td>US, NY, New York</td>\n",
       "      <td>Marketing</td>\n",
       "      <td></td>\n",
       "      <td>We're Food52, and we've created a groundbreaki...</td>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>Experience with content management systems a m...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Internship</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Marketing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Customer Service - Cloud Video Production</td>\n",
       "      <td>NZ, , Auckland</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>90 Seconds, the worlds Cloud Video Production ...</td>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>What we expect from you:Your key responsibilit...</td>\n",
       "      <td>What you will get from usThrough being part of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td></td>\n",
       "      <td>Marketing and Advertising</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Commissioning Machinery Assistant (CMA)</td>\n",
       "      <td>US, IA, Wever</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Valor Services provides Workforce Solutions th...</td>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>Implement pre-commissioning and commissioning ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Account Executive - Washington DC</td>\n",
       "      <td>US, DC, Washington</td>\n",
       "      <td>Sales</td>\n",
       "      <td></td>\n",
       "      <td>Our passion for improving quality of life thro...</td>\n",
       "      <td>THE COMPANY: ESRI ‚Äì Environmental Systems Rese...</td>\n",
       "      <td>EDUCATION:¬†Bachelor‚Äôs or Master‚Äôs in GIS, busi...</td>\n",
       "      <td>Our culture is anything but corporate‚Äîwe have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Computer Software</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bill Review Manager</td>\n",
       "      <td>US, FL, Fort Worth</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SpotSource Solutions LLC is a Global Human Cap...</td>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>QUALIFICATIONS:RN license in the State of Texa...</td>\n",
       "      <td>Full Benefits Offered</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Hospital &amp; Health Care</td>\n",
       "      <td>Health Care Provider</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id                                      title            location  \\\n",
       "0       1                           Marketing Intern    US, NY, New York   \n",
       "1       2  Customer Service - Cloud Video Production      NZ, , Auckland   \n",
       "2       3    Commissioning Machinery Assistant (CMA)       US, IA, Wever   \n",
       "3       4          Account Executive - Washington DC  US, DC, Washington   \n",
       "4       5                        Bill Review Manager  US, FL, Fort Worth   \n",
       "\n",
       "  department salary_range                                    company_profile  \\\n",
       "0  Marketing               We're Food52, and we've created a groundbreaki...   \n",
       "1    Success               90 Seconds, the worlds Cloud Video Production ...   \n",
       "2                          Valor Services provides Workforce Solutions th...   \n",
       "3      Sales               Our passion for improving quality of life thro...   \n",
       "4                          SpotSource Solutions LLC is a Global Human Cap...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Food52, a fast-growing, James Beard Award-winn...   \n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
       "2  Our client, located in Houston, is actively se...   \n",
       "3  THE COMPANY: ESRI ‚Äì Environmental Systems Rese...   \n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
       "\n",
       "                                        requirements  \\\n",
       "0  Experience with content management systems a m...   \n",
       "1  What we expect from you:Your key responsibilit...   \n",
       "2  Implement pre-commissioning and commissioning ...   \n",
       "3  EDUCATION:¬†Bachelor‚Äôs or Master‚Äôs in GIS, busi...   \n",
       "4  QUALIFICATIONS:RN license in the State of Texa...   \n",
       "\n",
       "                                            benefits  telecommuting  \\\n",
       "0                                                                 0   \n",
       "1  What you will get from usThrough being part of...              0   \n",
       "2                                                                 0   \n",
       "3  Our culture is anything but corporate‚Äîwe have ...              0   \n",
       "4                              Full Benefits Offered              0   \n",
       "\n",
       "   has_company_logo  has_questions employment_type required_experience  \\\n",
       "0                 1              0           Other          Internship   \n",
       "1                 1              0       Full-time      Not Applicable   \n",
       "2                 1              0                                       \n",
       "3                 1              0       Full-time    Mid-Senior level   \n",
       "4                 1              1       Full-time    Mid-Senior level   \n",
       "\n",
       "  required_education                   industry              function  \\\n",
       "0                                                           Marketing   \n",
       "1                     Marketing and Advertising      Customer Service   \n",
       "2                                                                       \n",
       "3  Bachelor's Degree          Computer Software                 Sales   \n",
       "4  Bachelor's Degree     Hospital & Health Care  Health Care Provider   \n",
       "\n",
       "   fraudulent  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"fake_job_postings.xlsx\")\n",
    "df = df.fillna(\" \")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this dataset has a number of columns, since the goal of this is to utilize the text information, we'll select only the columns we actually want to utilize to keep our dataset a bit more tidy.  We'll stick with leveraging the description and requirements fields as we are hoping group these jobs into similar types of jobs based on the text of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>Experience with content management systems a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>What we expect from you:Your key responsibilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>Implement pre-commissioning and commissioning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI ‚Äì Environmental Systems Rese...</td>\n",
       "      <td>EDUCATION:¬†Bachelor‚Äôs or Master‚Äôs in GIS, busi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>QUALIFICATIONS:RN license in the State of Texa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Food52, a fast-growing, James Beard Award-winn...   \n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
       "2  Our client, located in Houston, is actively se...   \n",
       "3  THE COMPANY: ESRI ‚Äì Environmental Systems Rese...   \n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
       "\n",
       "                                        requirements  \n",
       "0  Experience with content management systems a m...  \n",
       "1  What we expect from you:Your key responsibilit...  \n",
       "2  Implement pre-commissioning and commissioning ...  \n",
       "3  EDUCATION:¬†Bachelor‚Äôs or Master‚Äôs in GIS, busi...  \n",
       "4  QUALIFICATIONS:RN license in the State of Texa...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_skinny = df[['description', 'requirements']]\n",
    "df_skinny.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "Vectorizing is just a fancy word for \"splitting apart\" the words from the text into columns.  We will have a number of parameters that must be defined by us to get the algorithm to function.\n",
    "\n",
    "* ngram_range\n",
    "* max_df and min_df\n",
    "* stop_words\n",
    "\n",
    "But first, we need to understand the two primary types of vectorizing.  The first and easiest to understand is the count vectorizor. Every time the word or combination of words appears in the text, it is given a count of 1.  If the same word shows up 3 times, it shows as a count of 3.  In practice this is rarely used as the TFIDF vectorizor will almost always produce superior performance.  TFIDF stands for \"Term Frequency Inverse Document Frequency\".  While the math behind this is beyond the scope of this resource, know that TFIDF is trying to account for how important the word is to the document.  For example, if you work for Amazon and are analyzing your job profile text, the word Amazon is relatively unimportant given its the company name.  However, if you work for a Walmart and are doing the same exercise, Amazon may be more important as it could relate to AWS.  In short, use the TFIDF vectorizor because you'll get better results.\n",
    "\n",
    "Within the TFIDF vectorizor, we'll utilize the various parameters I mentioned above.  Let's talk about each of them a bit prior to diving into the TFIDF function.\n",
    "\n",
    "N-grams are a how many words you want to be strung together.  For example, if your n-gram range is only 1 to 1, then you'd only be getting each word by itself.  While this can be helpful, you will likely lose a lot of important context by only looking at each word in isolation.  If you set your n-gram range as 1 to 3, you'll get each single, pair, and trio of words.  This helps retain some more of the context of your text when multiple words that are next to each other in the sentence can be kept together throughout your vectorization process.\n",
    "\n",
    "The min and max df parameters ensure very common and very unique words don't stay in your dataset for consideration.  The value input for each of these is a percentage.  For example, if min_df is set to .05, that means the word has to show up in at least 5% of your documents to be kept in.  For max_df, if it is set to .95, that means any words in more than 95% of your documents are removed.  \n",
    "\n",
    "The last parameter isn't much of an option.  For stop_words, you want to remove the stop words from your comments.  Currently sklearn only supports English as of this writing.\n",
    "\n",
    "Now onto our example.  Since we're using multiple columns, this will require some programming to execute the process on each of our columns independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of the 2 column positions for the dataset\n",
    "col_nums = [0,1]\n",
    "\n",
    "#create a blank dataframe so we can put the vectorized columns in it\n",
    "df_vect = pd.DataFrame()\n",
    "\n",
    "#set vectorizer parameters\n",
    "tfidf_vec = TfidfVectorizer(stop_words = 'english', ngram_range = (1,3), max_df = 0.75, min_df = .005)\n",
    "\n",
    "#run vectorization process on each column\n",
    "for col in col_nums:\n",
    "    \n",
    "    #select the single column\n",
    "    temp_df = df_skinny[df_skinny.columns[col]]\n",
    "    \n",
    "    #execute vectorizor on data\n",
    "    after_tfidf = tfidf_vec.fit_transform(temp_df)\n",
    "    \n",
    "    #get the column names\n",
    "    col_names = tfidf_vec.get_feature_names()\n",
    "    \n",
    "    #convert into df\n",
    "    after_tfidf = pd.DataFrame(after_tfidf.toarray(), columns = col_names)\n",
    "    \n",
    "    #put together each iteration of the vectorization process into a single dataframe\n",
    "    df_vect = pd.concat([df_vect, after_tfidf], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, we are now able to see that our dataset has 6001 columns.  Each column represents a word or consecutive string of words and the number is how important the word(s) is/are to the text.\n",
    "\n",
    "At least for me, this is where I can more clearly see how a computer or model can utilize text information to understand it.  We've taken a bunch of words and turned them into a tabular dataset that can go into a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17880, 6001)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>10 years</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>12 month</th>\n",
       "      <th>12 month contract</th>\n",
       "      <th>...</th>\n",
       "      <th>years professional</th>\n",
       "      <th>years professional experience</th>\n",
       "      <th>years related</th>\n",
       "      <th>years relevant</th>\n",
       "      <th>years sales</th>\n",
       "      <th>years software</th>\n",
       "      <th>years work</th>\n",
       "      <th>years work experience</th>\n",
       "      <th>years working</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 6001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00       000   10  10 years       100  1000   11   12  12 month  \\\n",
       "0  0.0  0.000000  0.0       0.0  0.000000   0.0  0.0  0.0       0.0   \n",
       "1  0.0  0.106598  0.0       0.0  0.056939   0.0  0.0  0.0       0.0   \n",
       "2  0.0  0.000000  0.0       0.0  0.000000   0.0  0.0  0.0       0.0   \n",
       "3  0.0  0.000000  0.0       0.0  0.000000   0.0  0.0  0.0       0.0   \n",
       "4  0.0  0.000000  0.0       0.0  0.000000   0.0  0.0  0.0       0.0   \n",
       "\n",
       "   12 month contract  ...  years professional  years professional experience  \\\n",
       "0                0.0  ...                 0.0                            0.0   \n",
       "1                0.0  ...                 0.0                            0.0   \n",
       "2                0.0  ...                 0.0                            0.0   \n",
       "3                0.0  ...                 0.0                            0.0   \n",
       "4                0.0  ...                 0.0                            0.0   \n",
       "\n",
       "   years related  years relevant  years sales  years software  years work  \\\n",
       "0            0.0             0.0          0.0             0.0         0.0   \n",
       "1            0.0             0.0          0.0             0.0         0.0   \n",
       "2            0.0             0.0          0.0             0.0         0.0   \n",
       "3            0.0             0.0          0.0             0.0         0.0   \n",
       "4            0.0             0.0          0.0             0.0         0.0   \n",
       "\n",
       "   years work experience  years working  yes  \n",
       "0                    0.0            0.0  0.0  \n",
       "1                    0.0            0.0  0.0  \n",
       "2                    0.0            0.0  0.0  \n",
       "3                    0.0            0.0  0.0  \n",
       "4                    0.0            0.0  0.0  \n",
       "\n",
       "[5 rows x 6001 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vect.shape)\n",
    "df_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next question is, what do you do with this information?\n",
    "\n",
    "## What the heck do I do with 6000 columns?\n",
    "By themselves, this many columns are pretty useless.  With most basic modeling or analytics techniques, having this many variables becomes a problem.  Each column by themselves don't tell us much, but when you can start interacting the columns together, you may start getting somewhere.  You can also leverage dimensionality reduction techniques to reduce the number of variables you have.  This will allow you to utilize more traditional analysis or modeling techniques.  I'll speak about two paths you can take to put this text analysis into action.  As always, it should go back to your use case on what to do.\n",
    "\n",
    "1. Topic Modeling: If you're familiar with Principle Component Analysis or other dimensionality reduction techniques, this is a similar concept.  Reduce the number of variables while minimizing information loss.\n",
    "\n",
    "2. Build Tree-Based Model: One significant benefit of tree-based models is how well it can handle high dimension datasets.  Feeding a random forest or GBM 6000 columns won't cause any issues because the nature of a tree based model will be to ignore columns that don't contribute to the prediction.  This approach can also use a topic model, which may produce a better or worse result depending on your data.\n",
    "\n",
    "#### Topic Modeling\n",
    "Topic modeling is a dimensionality reduction technique that can be a way to identify themes or topics within text data.  This is an unsupervised approach.  You input how many topics to create and the output is X number of columns with a probability that text fits into the topic.  Unfortunately the magic stops here, there isn't a name for the topic provided.  A common approach is for an analyst to review the words or phrases within that topic to name it.  If you're familiar with clustering, this is similar.  The clusters are created, but they still need to be provided a name.  \n",
    "\n",
    "In the HR space, building a topic model has many uses.  From an employee survey perspective, any open comments can then be grouped via a topic model to get a general idea of what employees are discussing.  If you pair this with your more standard HR data elements like tenure, job level, age, department, etc, this can provide a new perspective on your survey data you didn't have previously.  With your performance goal write ups, you could use a topic model to identify the themes of goals across teams and/or departments within your company.\n",
    "\n",
    "#### Build Tree-Based Model\n",
    "If you're hoping to build a predictive model, you can build a tree-based model (decision tree, random forest, GBM) using a high number of features that get output from the vectorization process.  You have many options on how to approach this.  You can simply use all the vectorized column to predict whatever you're trying to predict, you can add in other more standard HR data elements (tenure, age, job level, etc), you can put your vectorized data through a topic model, or you can do any combination of the three.  As long as you have time, its best to try the various approaches to see which provides the best results!\n",
    "\n",
    "In the HR space, a good example for this is predicting turnover.  Utilizing your structured data, survey comments, performance reviews, peer feedback, etc all together will almost certainly provide a superior result than each by itself.  Leveraging a tree-based model to find some of these complex interactions will be something regression cannot solve for without a signficant amount of additional time and complexity in the modeling process.\n",
    "\n",
    "## Topic Model Example\n",
    "There are a handful of different approaches to topic modeling.  In this example we'll use LSA or Latent Sematic Analysis.  It is using SVD, or Singular Value Decomposition, however since we used TFIDF to vectorize our data, its considered LSA. Know that this is using linear algebra on the backend, but the actual math behind this is beyond the scope of this chapter.  Once you've done all the work to get your text vectorized, you'll find that topic modeling is quite easy!  Topic modeling definitely fits into the classic analysis stereotype of most time is spent prepping the data rather than with the actual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = TruncatedSVD(n_components = 5)\n",
    "topic_model_df = topic_model.fit_transform(df_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the topic model is as easy as that!  I've found it valuable to play with how many topics are created.  Make sure to use your use case as context to help inform that.  If you're trying to identify themes from an onboarding survey, its not likely that 25 themes will be helpful!  If you're just using the topics to be fed into a model, the actual meaning of the topics aren't as important as the means to reduce the number of columns you're feeding into your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.256389</td>\n",
       "      <td>-0.062560</td>\n",
       "      <td>-0.026146</td>\n",
       "      <td>-0.016824</td>\n",
       "      <td>-0.025928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.385037</td>\n",
       "      <td>-0.086439</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>-0.014787</td>\n",
       "      <td>-0.040057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.149374</td>\n",
       "      <td>-0.037078</td>\n",
       "      <td>0.036085</td>\n",
       "      <td>-0.002194</td>\n",
       "      <td>0.029768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440844</td>\n",
       "      <td>-0.097447</td>\n",
       "      <td>-0.049344</td>\n",
       "      <td>-0.003590</td>\n",
       "      <td>-0.050383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.294612</td>\n",
       "      <td>-0.063121</td>\n",
       "      <td>0.042366</td>\n",
       "      <td>-0.006927</td>\n",
       "      <td>0.018472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  0.256389 -0.062560 -0.026146 -0.016824 -0.025928\n",
       "1  0.385037 -0.086439  0.000473 -0.014787 -0.040057\n",
       "2  0.149374 -0.037078  0.036085 -0.002194  0.029768\n",
       "3  0.440844 -0.097447 -0.049344 -0.003590 -0.050383\n",
       "4  0.294612 -0.063121  0.042366 -0.006927  0.018472"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_df = pd.DataFrame(topic_model_df)\n",
    "topic_model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number represents a probability that text relates to the comments.  There are two important things to note.\n",
    "\n",
    "1. If using the numbers, take the absolute value.  A negative number doesn't mean less of a chance the text is related to a topic.\n",
    "\n",
    "2. The numbers do not add up to 100.  This is because the process is accounting for some uncertainty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
